{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import requests\n",
    "import igraph as ig\n",
    "import sys\n",
    "import pickle\n",
    "from pytictoc import TicToc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import reduce\n",
    "import itertools\n",
    "from itertools import chain\n",
    "import cairocffi as cairo\n",
    "import networkx as nx\n",
    "import csv\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Thic script consists of three steps,\n",
    "every step is seperated with ### check point ####\n",
    "the input is batch_task and the output is dictionaries needed for further analysis\n",
    "the rule based classification is also determined here. \n",
    "no prediction of runtime is applied here\n",
    "\n",
    "\n",
    "till first checkpoint data manipulating, runtime ca. 1 hour\n",
    "till second checkpoint further data manipulating runtime < 30 minitues\n",
    "third checkpoint: highly time consuming (more than 2 days), the igraph objects extraction, rule-based classification, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of nodes withing a string of tasks\n",
    "def nodeCounter(dags):\n",
    "    temp = re.split(\",|§\",dags)\n",
    "    temp = ' '.join(temp).replace('-','').split()\n",
    "    return (len(set(temp)))\n",
    "\n",
    "\n",
    "#extracting the edges from string of tasks in desired format of igraph\n",
    "def edgeExtractor (df):\n",
    "    fr = []\n",
    "    edges = df[df[\"task_name\"].str.contains(\"-\")][\"task_name\"].to_list()\n",
    "    for i in edges:\n",
    "        templist = list (map (int, i.split(\"-§-\"))) \n",
    "        templist = list (map (lambda x: x - 1, templist))\n",
    "        a, b = [templist[0]], templist[1:]\n",
    "        c = list(itertools.product(b,a))\n",
    "        fr.append(c)\n",
    "    fr = list(chain.from_iterable(fr))\n",
    "    return fr\n",
    "\n",
    "\n",
    "#eliminating the islands of a graph\n",
    "def isolatedDeletor (g):\n",
    "    to_delete_ids = [v.index for v in g.vs if v.degree() == 0]\n",
    "    g.delete_vertices(to_delete_ids)\n",
    "    return g\n",
    "\n",
    "#sort a dataframe in a way that the attributes of tasks match to the nodes correctly\n",
    "def sorter (df):\n",
    "    df['node_root'] = df['task_name'].str.extract('(\\d+)').astype(int)\n",
    "    df['node_root'] = df['node_root']  -1\n",
    "    return df.sort_values(by = ['node_root'])\n",
    "\n",
    "\n",
    "#return lists of attributes\n",
    "def attExtractor (df):\n",
    "    return df.plan_cpu.to_list(), df.plan_mem.to_list(), df.instance_num.to_list()\n",
    "\n",
    "\n",
    "# simply the graph object (eiminate the duplicate edges)\n",
    "def simplifier (g):  \n",
    "    simpleG = g.simplify()\n",
    "    return simpleG\n",
    "\n",
    "#highlighting the relevant time intervals\n",
    "secondsMin, secondsHour, secondsDay = [],[],[]\n",
    "def lablingfunc(x):\n",
    "    starttime = DAG.loc[DAG['job_name'] == x ,\"totalStart\"]\n",
    "    if starttime.isin(secondsMin).bool(): x = x + \" M\"\n",
    "    if starttime.isin(secondsHour).bool(): x = x + \" H\"\n",
    "    if starttime.isin(secondsDay).bool(): x = x + \" D\"\n",
    "     \n",
    "    return x \n",
    "\n",
    "\n",
    "#selecting only isomorphic graphs.\n",
    "def filterfunc(x):\n",
    "    return DAGDict[x].isomorphic(DAGDict[check])\n",
    "\n",
    "\n",
    "#return all time intervals of interest i.e 900 sec(15 min), 3600(hourly), 86400(daily)\n",
    "#considers also a 6% margins\n",
    "def secondExtractor(second):\n",
    "    \n",
    "    mins = list(range(second,maxTime,900))\n",
    "   # mins.pop(0)\n",
    "    minsMax = list(map(lambda x : x + (9*3), mins)) \n",
    "    minsMin = list(map(lambda x : x - (9*3), mins)) \n",
    "    \n",
    "    hours = list(range(second,maxTime,3600))\n",
    "  #  hours.pop(0)\n",
    "    hoursMax = list(map(lambda x : x + (36*3), hours))\n",
    "    hoursMin = list(map(lambda x : x - (36*3), hours)) \n",
    "    \n",
    "    days = list(range(second,maxTime,86400))\n",
    "  #  days.pop(0)\n",
    "    daysMax = list(map(lambda x : x + (864*3), days)) \n",
    "    daysMin = list(map(lambda x : x - (864*3), days)) \n",
    "    \n",
    "    MaxInt = minsMax + hoursMax + daysMax\n",
    "    MinInt = minsMin + hoursMin + daysMin\n",
    "    \n",
    "        \n",
    "    secondsMin =  list()   \n",
    "    for i,j in zip(minsMin,minsMax) : #creating only minute by minute intervals (every 15 minitues)\n",
    "        tempInt = []\n",
    "        tempInt = list(range(i,j))\n",
    "        secondsMin.extend(tempInt)\n",
    "\n",
    "        \n",
    "    secondsHour =  list()\n",
    "    for i,j in zip(hoursMin,hoursMax) : #creating only hourly intervals \n",
    "        tempInt = []\n",
    "        tempInt = list(range(i,j))\n",
    "        secondsHour.extend(tempInt)\n",
    "\n",
    "        \n",
    "    secondsDay =  list()\n",
    "    for i,j in zip(daysMin,daysMax) : #creating only daily intervals\n",
    "        tempInt = []\n",
    "        tempInt = list(range(i,j))\n",
    "        secondsDay.extend(tempInt)\n",
    "        \n",
    "    secondsTotal = list()\n",
    "    secondsTotal =  secondsMin + secondsHour + secondsDay\n",
    "    \n",
    "    return secondsTotal, secondsMin, secondsHour, secondsDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "time=TicToc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/batch_task.csv\",sep =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the data: (14295731, 9)\n",
      "number of jobs: 4201014\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of the data:\", df.shape)\n",
    "print(\"number of jobs:\", len(set(df.job_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['task_name', 'instance_num', 'job_name', 'task_type', 'status',\n",
       "       'start_time', 'end_time', 'plan_cpu', 'plan_mem'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning runtime, deleting unfinished jobs\n",
    "df[\"Duration\"] = df[\"end_time\"] - df[\"start_time\"]\n",
    "#deleting tasks that are not complete\n",
    "df[\"task_name\"] = df[\"task_name\"].apply(lambda x :x[:-1] if x[-1] is '_' else x)\n",
    "#transferring independent tasks to readble tasks\n",
    "df.loc[(df[\"task_name\"].str.startswith(\"task\")),\"task_name\"]= (np.arange((df[\"task_name\"].str.startswith(\"task\")).sum()) + 1) + 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing data for generating DAGs\n",
    "df[\"task_name\"]=df.task_name.str.replace(\"[^0-9._]\", \"\")\n",
    "df[\"task_name\"]=df.task_name.str.replace(\"\\\\_\", \"-§-\")\n",
    "df = df.dropna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DAGs'] = df[['job_name','task_name']].groupby(['job_name'])['task_name'].transform(lambda x: ','.join(x))\n",
    "df['totalStart'] = df[['job_name','start_time']].groupby(['job_name'])['start_time'].transform(lambda x: min(x))\n",
    "df['totalEnd'] = df[['job_name','end_time']].groupby(['job_name'])['end_time'].transform(lambda x: max(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11892803"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(0)\n",
    "df = sorter (df)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(df.job_name))\n",
    "#count node with two descriptions\n",
    "df['node'] = df['task_name'].str[0]\n",
    "without_two_description = df[['job_name', 'node']].drop_duplicates(keep = False)\n",
    "with_two_description = df[~df.job_name.isin(without_two_description.job_name)]\n",
    "print('# jobs with two descriptions:',len(set(with_two_description.job_name)))\n",
    "\n",
    "#counting the number of jobs which have lesser than ten nodes\n",
    "jobs = df[['job_name','DAGs','totalStart','totalEnd']].drop_duplicates()\n",
    "jobs[\"nNodes\"] = jobs.DAGs.apply(nodeCounter)\n",
    "jobs = jobs[jobs.nNodes<=9]\n",
    "print('# lesser than ten nodes',len(jobs))\n",
    "\n",
    "##counting the number of jobs without dependencies\n",
    "DAG_without_dependencies = df[~df[\"DAGs\"].str.contains(\"§\")]\n",
    "print('#without dependencies', len(set(DAG_without_dependencies.job_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAG = df[['job_name','DAGs','totalStart','totalEnd']].drop_duplicates()\n",
    "DAG = DAG[DAG[\"DAGs\"].str.contains(\"§\")]\n",
    "DAG['run_time']=DAG['totalEnd'] - DAG['totalStart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAG[\"nNodes\"]= 0\n",
    "DAG[\"nNodes\"] = DAG.DAGs.apply(nodeCounter)\n",
    "DAG = DAG[DAG[\"nNodes\"] > 9]\n",
    "DAG = DAG.sort_values(\"totalStart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250076"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetJobs = list(set(DAG.job_name))\n",
    "len(targetJobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#DAG.to_csv(\"data/preprocessed_tasks_v2.csv\",sep =',')\n",
    "DAG = pd.read_csv('data/preprocessed_tasks_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating dictionary of igraph objects for jobs\n",
    "#targetJobs = list(set(DAG.job_name))\n",
    "#highly time consuming ca. three days\n",
    "targetJobs = list(set(DAG.job_name))\n",
    "len(targetJobs)\n",
    "time.tic()\n",
    "DagsDict = {}\n",
    "for x in targetJobs:\n",
    "    tempdf = df[df.job_name == x]\n",
    "    tempdf = sorter (tempdf)\n",
    "    tempg = ig.Graph(edgeExtractor(tempdf),directed=True)\n",
    "    tempg.vs[\"plan_cpu\"], tempg.vs[\"plan_mem\"], tempg.vs[\"instance_num\"] = attExtractor(tempdf)\n",
    "    DagsDict.update({x : tempg})\n",
    "time.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igraphObjects = {k: isolatedDeletor(v) for k, v in DagsDict.items()}\n",
    "DagsDictPruned = {k: simplifier(v) for k, v in igraphObjects.items()}\n",
    "networkXObjects = {key: value.to_networkx() for key, value in igraphObjects.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dictionary for job. key is job id and value is list of interesting jobs\n",
    "#time consuming loop\n",
    "time.tic()\n",
    "maxTime = max(DAG[\"totalEnd\"])\n",
    "isomorphicDict = {}\n",
    "mutable = len(DAG)\n",
    "\n",
    "#for y in range(len(DAG)):\n",
    "for y in range(10):\n",
    "    secondsTotal, secondsMin, secondsHour, secondsDay = [], [], [], []\n",
    "    secondsTotal, secondsMin, secondsHour, secondsDay = secondExtractor(DAG.iloc[y,2])\n",
    "\n",
    "\n",
    "    \n",
    "    listIsomorphic = list()\n",
    "    \n",
    "    target = DAG[DAG['totalStart'].isin(secondsTotal)]\n",
    "    target = list(target['job_name'])\n",
    "    \n",
    "    \n",
    "    check = DAG.iloc[y,0]\n",
    "    \n",
    "    recurrentJobs = list()\n",
    "    recurrentJobs = list(filter(f, target))\n",
    "    recurrentJobs = map(lablingfunc,recurrentJobs)\n",
    "     \n",
    "    \n",
    "    isomorphicDict.update({DAG.iloc[y,0] : list(recurrentJobs)})    \n",
    "\n",
    "time.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### transformation of DAG df. \n",
    "#adding assigned group to jobs base on time interval and isomorphic.\n",
    "#detecting to which time interval they belong.\n",
    "#!!time consuming loop, the results saved in ruleBased_DAGs.csv\n",
    "\n",
    "### tempppp\n",
    "DAG['IfMins'] = False\n",
    "DAG['IfHours'] = False \n",
    "DAG['IfDays'] = False\n",
    "\n",
    "DAG['mutable'] = True\n",
    "DAG['group'] = 0\n",
    "\n",
    "time.tic()\n",
    "maxTime = max(DAG[\"totalEnd\"])\n",
    "isomorphicDict = {}\n",
    "\n",
    "\n",
    "for y in range(len(DAG)) :\n",
    "    \n",
    "    secondsTotal, secondsMin, secondsHour, secondsDay = [], [], [], []\n",
    "    secondsTotal, secondsMin, secondsHour, secondsDay = secondExtractor(DAG.iloc[y,2])\n",
    "\n",
    "\n",
    "    \n",
    "    listIsomorphic = list()\n",
    "    \n",
    "    target = DAG[DAG['totalStart'].isin(secondsTotal)]\n",
    "    target = target[target['mutable'] == True]\n",
    "    target = list(target['job_name'])\n",
    "    \n",
    "    check = DAG.iloc[y,0]\n",
    "    \n",
    "    recurrentJobs = list()\n",
    "    recurrentJobs = list(filter(filterfunc, target))\n",
    "    \n",
    "    DAG.loc[(DAG.job_name.isin(recurrentJobs)) & (DAG.mutable == True), 'group'] = y\n",
    "    \n",
    "    DAG.loc[(DAG.group == y) & (DAG.totalStart.isin(secondsMin)), 'IfMins'] = True\n",
    "    DAG.loc[(DAG.group == y) & (DAG.totalStart.isin(secondsHour)), 'IfHours'] = True\n",
    "    DAG.loc[(DAG.group == y) & (DAG.totalStart.isin(secondsDay)), 'IfDays'] = True\n",
    "    \n",
    "    DAG.loc[(DAG.job_name.isin(recurrentJobs)) & (DAG.mutable == True), 'mutable'] = False \n",
    "\n",
    "time.toc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolatedDeletor (g):\n",
    "    to_delete_ids = [v.index for v in g.vs if v.degree() == 0]\n",
    "    g.delete_vertices(to_delete_ids)\n",
    "    return g\n",
    "d2 = {k: isolatedDeletor(v) for k, v in DagsDict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_to_write = open(\"data/DAGs_igraph_dict.pickle\", \"wb\")\n",
    "#pickle.dump(d2, file_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DAG.to_csv(\"data/ruleBased_DAGs.csv\",sep =',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('data/ruleBased_DAGs_dict.npy', isomorphicDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch.to_csv(\"data/preprocessed_jobs.csv\",sep =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####check point########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_read = open(\"data/DAGs_igraph_dict.pickle\", \"rb\")\n",
    "DAGs_Dict = pickle.load(file_to_read)\n",
    "DAG = pd.read_csv(\"data/ruleBased_DAGs.csv\",sep =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring the groups\n",
    "#size of groups of isomorohic DAGs\n",
    "read_dictionary = np.load('data/ruleBased_DAGs_dict.npy',allow_pickle='TRUE').item()\n",
    "length_dict = {key: len(value) for key, value in read_dictionary.items()}\n",
    "lengths = [len(v) for v in read_dictionary.values()]\n",
    "sns.boxplot(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1094936.3586614884"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DAG.groupby('group')['run_time'].var().mean()\n",
    "print('runtima variation within every group',DAG.run_time.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206517 from  250076  jobs more than 1 recurrent jobs\n",
      "192527 from  250076  jobs more than 2 recurrent jobs\n",
      "181479 from  250076  jobs more than 3 recurrent jobs\n",
      "170335 from  250076  jobs more than 4 recurrent jobs\n",
      "161959 from  250076  jobs more than 5 recurrent jobs\n",
      "153458 from  250076  jobs more than 6 recurrent jobs\n",
      "149010 from  250076  jobs more than 7 recurrent jobs\n",
      "143972 from  250076  jobs more than 8 recurrent jobs\n",
      "140089 from  250076  jobs more than 9 recurrent jobs\n",
      "135985 from  250076  jobs more than 10 recurrent jobs\n",
      "132794 from  250076  jobs more than 11 recurrent jobs\n",
      "128672 from  250076  jobs more than 12 recurrent jobs\n",
      "125806 from  250076  jobs more than 13 recurrent jobs\n",
      "123021 from  250076  jobs more than 14 recurrent jobs\n",
      "120198 from  250076  jobs more than 15 recurrent jobs\n",
      "117313 from  250076  jobs more than 16 recurrent jobs\n",
      "114852 from  250076  jobs more than 17 recurrent jobs\n",
      "112331 from  250076  jobs more than 18 recurrent jobs\n",
      "110148 from  250076  jobs more than 19 recurrent jobs\n",
      "107785 from  250076  jobs more than 20 recurrent jobs\n",
      "105711 from  250076  jobs more than 21 recurrent jobs\n",
      "103648 from  250076  jobs more than 22 recurrent jobs\n",
      "101574 from  250076  jobs more than 23 recurrent jobs\n",
      "99424 from  250076  jobs more than 24 recurrent jobs\n",
      "97598 from  250076  jobs more than 25 recurrent jobs\n",
      "95814 from  250076  jobs more than 26 recurrent jobs\n",
      "94126 from  250076  jobs more than 27 recurrent jobs\n",
      "92445 from  250076  jobs more than 28 recurrent jobs\n",
      "90848 from  250076  jobs more than 29 recurrent jobs\n"
     ]
    }
   ],
   "source": [
    "number = sum(i >= 0 for i in lengths)\n",
    "for j in range(1,30):\n",
    "    print ( sum(i > j for i in lengths),\"from \" ,number, \" jobs more than\" , j, \"recurrent jobs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
